{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lRC5Mk7DS7il"
   },
   "source": [
    "# BiLSTM with CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "colab_type": "code",
    "id": "s7pqLoSXKbh6",
    "outputId": "0ca5ebdf-b819-4a70-9098-978868d85385"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torchtext.legacy.data import Field, NestedField, BucketIterator\n",
    "from torchtext.legacy.datasets import SequenceTaggingDataset\n",
    "from torchtext.vocab import Vocab\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy import data\n",
    "from torchtext import datasets\n",
    "from torchcrf import CRF\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np \n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k4LF_qyLKvfK"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the seed value for reproducible set in future\n",
    "seed_value = 42\n",
    "\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "text_data = data.Field(lower = False) \n",
    "\n",
    "# the tags are all known so we have unk_token = None\n",
    "tags = data.Field(unk_token = None)\n",
    "\n",
    "# reading data files in the given directory\n",
    "path_str = \"data/\"\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "        path= path_str ,\n",
    "        train=\"train.csv\",\n",
    "        validation=\"valid.csv\",\n",
    "        test=\"test.csv\", format='csv', skip_header=True,\n",
    "        fields=((\"text\", text_data), (\"tag\", tags))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JAamZSXiKnJl"
   },
   "outputs": [],
   "source": [
    "# keeping the min_frequency of the words to zero so that\n",
    "# words that appear less than MIN_FREQ times will be ignored from the vocabulary\n",
    "\n",
    "MIN_FREQ = 0\n",
    "\n",
    "# building vocab\n",
    "\n",
    "text_data.build_vocab(train_data, \n",
    "                 min_freq = MIN_FREQ, \n",
    "                 vectors = \"glove.6B.100d\",\n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "\n",
    "tags.build_vocab(train_data)\n",
    "\n",
    "# defining the batch size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "train_iterator, validation_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device, sort=False)\n",
    "\n",
    "# padding index\n",
    "TEXT_PAD_IDX = text_data.vocab.stoi[text_data.pad_token]  \n",
    "TAG_PAD_IDX = tags.vocab.stoi[tags.pad_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXY-SyrWKx4u"
   },
   "source": [
    "# Defining the model architecture\n",
    "\n",
    "\n",
    "Building the class for the model declaration with different layers of the model with names mentioned in the comments:\n",
    "\n",
    "1. To prepare the CRF layer during initialization, we need to specify the number of possible tags in the text.\n",
    "2. Adding CRF layer logic in the `forward()` sequence because of the major change in the implementation of the `pytorch-crf` package. Earlier, the forward spread and loss calculation was done separately but now the calculation of losses is integrated into the forward spread.\n",
    "3. Initializing all the impossible transitions with a really low number (-100) in the `init_crf_transitions` function.\n",
    "4. This is where the BIO logic (Begin, Inside and out) on sequence for the model is imposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PW8shLquKyr-"
   },
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Class to create the model with the desired shape and fixed architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 lstm_layers,\n",
    "                 emb_dropout,\n",
    "                 lstm_dropout,\n",
    "                 fc_dropout,\n",
    "                 word_pad_idx,\n",
    "                 tag_pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        # LAYER 1: Word Embeddings Layer\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=input_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=word_pad_idx\n",
    "        )\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "\n",
    "        # LAYER 2: BiLSTM Layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            bidirectional=True,\n",
    "            dropout=lstm_dropout if lstm_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # LAYER 3: Fully-connected Layer \n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "        # LAYER 4: CRF Layer\n",
    "        self.tag_pad_idx = tag_pad_idx\n",
    "        self.crf = CRF(num_tags=output_dim)\n",
    "\n",
    "        # initializing weights with a normal distribution \n",
    "        for name, param in self.named_parameters():\n",
    "            nn.init.normal_(param.data, mean=0, std=0.1)\n",
    "\n",
    "    def forward(self, words, tags=None):\n",
    "        \"\"\"\n",
    "        Forward pass method for the words\n",
    "        :param: words: words in text data\n",
    "        :param: tags: tags of the data\n",
    "        \"\"\"\n",
    "        print(words)\n",
    "        embedding_out = self.emb_dropout(self.embedding(words))\n",
    "        lstm_out, _ = self.lstm(embedding_out)\n",
    "        fc_out = self.fc(self.fc_dropout(lstm_out))\n",
    "\n",
    "        if tags is not None:\n",
    "            mask = tags != self.tag_pad_idx\n",
    "            crf_out = self.crf.decode(fc_out, mask=mask)\n",
    "            crf_loss = -self.crf(fc_out, tags=tags, mask=mask)\n",
    "        else:\n",
    "            crf_out = self.crf.decode(fc_out)\n",
    "            crf_loss = None\n",
    "\n",
    "        return crf_out , crf_loss\n",
    "\n",
    "\n",
    "\n",
    "    def init_crf_transitions(self, tag_names, imp_value=-100):\n",
    "        \"\"\"\n",
    "        Initialize the CRF transitions.\n",
    "        :param: tag_names:\n",
    "        :param: imp_value: importance value with default as -100\n",
    "        \"\"\"\n",
    "        num_tags = len(tag_names)\n",
    "        for i in range(num_tags):\n",
    "            tag_name = tag_names[i]\n",
    "            if tag_name[0] == \"I\" or tag_name == \"<pad>\":\n",
    "                torch.nn.init.constant_(self.crf.start_transitions[i], imp_value)\n",
    "        # impossible transitions O - I\n",
    "        tag_is = {}\n",
    "        for tag_position in (\"B\", \"I\", \"O\"):\n",
    "            tag_is[tag_position] = [i for i, tag in enumerate(tag_names) if tag[0] == tag_position]\n",
    "        impossible_transitions_position = {\n",
    "            \"O\": \"I\"\n",
    "\n",
    "        }\n",
    "        for from_tag, to_tag_list in impossible_transitions_position.items():\n",
    "            to_tags = list(to_tag_list)\n",
    "\n",
    "            for from_tag_i in tag_is[from_tag]:\n",
    "                for to_tag in to_tags:\n",
    "                    for to_tag_i in tag_is[to_tag]:\n",
    "\n",
    "                        torch.nn.init.constant_(\n",
    "                            self.crf.transitions[from_tag_i, to_tag_i], imp_value\n",
    "                        )\n",
    "        # impossible transitions between different types\n",
    "\n",
    "        impossible_transitions_tags = {\n",
    "            \"B\": \"I\",\n",
    "            \"I\": \"I\"\n",
    "        }\n",
    "        for from_tag, to_tag_list in impossible_transitions_tags.items():\n",
    "            to_tags = list(to_tag_list)\n",
    "            for from_tag_i in tag_is[from_tag]:\n",
    "                for to_tag in to_tags:\n",
    "                    for to_tag_i in tag_is[to_tag]:\n",
    "                        if tag_names[from_tag_i].split(\"-\")[1] != tag_names[to_tag_i].split(\"-\")[1]:\n",
    "                            torch.nn.init.constant_(\n",
    "                                self.crf.transitions[from_tag_i, to_tag_i], imp_value\n",
    "                            )\n",
    "\n",
    "    def count_total_parameters(self):\n",
    "        \"\"\"\n",
    "        Count the parameters of the model.\n",
    "        \"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    \"\"\"\n",
    "    Initialize weights for all the model parameters.\n",
    "    :param: m: model object\n",
    "    \"\"\"\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean = 0, std = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "id": "Zb03iYctNAUM",
    "outputId": "4be1d224-4f2d-4a36-b9ce-128b3ce019a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (embedding): Embedding(23626, 100, padding_idx=1)\n",
       "  (emb_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lstm): LSTM(100, 256, bidirectional=True)\n",
       "  (fc_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (crf): CRF(num_tags=10)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim=100\n",
    "tag_pad_idx=TAG_PAD_IDX\n",
    "model = BiLSTM(\n",
    "    input_dim=len(text_data.vocab),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=256,\n",
    "    output_dim=len(tags.vocab),\n",
    "    lstm_layers=1,\n",
    "    emb_dropout=0.1,\n",
    "    lstm_dropout=0.1,\n",
    "    fc_dropout=0.1,\n",
    "    word_pad_idx=TEXT_PAD_IDX,\n",
    "    tag_pad_idx=TAG_PAD_IDX\n",
    ")\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Model has 3,101,034 parameters for training\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = text_data.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "model.embedding.weight.data[tag_pad_idx] = torch.zeros(embedding_dim)\n",
    "\n",
    "\n",
    "# CRF transitions initialisation\n",
    "model.init_crf_transitions(\n",
    "    tag_names=tags.vocab.itos\n",
    ")\n",
    "print(f\"The Model has {model.count_total_parameters():,} parameters for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "BiLSTM                                   --\n",
       "├─Embedding: 1-1                         2,362,600\n",
       "├─Dropout: 1-2                           --\n",
       "├─LSTM: 1-3                              733,184\n",
       "├─Dropout: 1-4                           --\n",
       "├─Linear: 1-5                            5,130\n",
       "├─CRF: 1-6                               120\n",
       "=================================================================\n",
       "Total params: 3,101,034\n",
       "Trainable params: 3,101,034\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5lWbTw3yMiyr"
   },
   "source": [
    "# Training\n",
    "\n",
    "The outputs of the model have two lists -> the predicted values and the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used the learning rate, epsilon, decay rates from other implementation\n",
    "\n",
    "def optimizer(model, lr=1e-5, eps=1e-6, weight_decay_rate=0.001, second_weight_decay_rate=0.0):\n",
    "    \"\"\"\n",
    "    Optimize the model using the Adam optimizer with given set values of parameters.\n",
    "    :param model: model object\n",
    "    :param lr: learning rate value\n",
    "    :param eps: epsilon value\n",
    "    :param weight_decay_rate: \n",
    "    :param second_weight_decay_rate: \n",
    "    :return: Adam optimizer object\n",
    "    \"\"\"\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': weight_decay_rate},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': second_weight_decay_rate}]\n",
    "    return optim.Adam(\n",
    "        optimizer_parameters,\n",
    "        lr=lr,\n",
    "        eps=eps\n",
    "    )\n",
    "optimizer_obj = optimizer(model, lr=1e-5, eps=1e-6, weight_decay_rate=0.001)\n",
    "scheduler_obj = optim.lr_scheduler.StepLR(optimizer_obj, step_size=10, gamma=0.5)\n",
    "criterion_func = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining metrics method for comparing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_loss( preds, y, tag_pad_idx, full_report=False):\n",
    "    \"\"\"\n",
    "    Compute the F1 Score for the predicted values.\n",
    "    :param preds: predicted tags for the given text.\n",
    "    :param y: actual tags for the given text.\n",
    "    :param tag_pad_idx: tag padding index\n",
    "    :param full_report: \n",
    "    :return: f1_score, flattened prediction, falttened actual values\n",
    "    \"\"\"\n",
    "    index_o = tags.vocab.stoi[\"O\"]\n",
    "    positive_labels = [i for i in range(len(tags.vocab.itos))\n",
    "                       if i not in (tag_pad_idx, index_o)]\n",
    "\n",
    "    flatten_preds = [pred for sent_pred in preds for pred in sent_pred]\n",
    "\n",
    "    positive_preds = [pred for pred in flatten_preds\n",
    "                      if pred not in (tag_pad_idx, index_o)]\n",
    "\n",
    "    flatten_y = [tag for sent_tag in y for tag in sent_tag]\n",
    "    if full_report:\n",
    "\n",
    "        positive_names = [tags.vocab.itos[i]\n",
    "                          for i in range(len(tags.vocab.itos))\n",
    "                          if i not in (tag_pad_idx, index_o)]\n",
    "        print(classification_report(\n",
    "            y_true=flatten_y,\n",
    "            y_pred=flatten_preds,\n",
    "            labels=positive_labels,\n",
    "            target_names=positive_names\n",
    "        ))\n",
    "\n",
    "    return f1_score(\n",
    "        y_true=flatten_y,\n",
    "        y_pred=flatten_preds,\n",
    "        labels=positive_labels,\n",
    "        average=\"micro\"\n",
    "    ), flatten_preds, flatten_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating methods for training, and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, tag_pad_idx):\n",
    "    \"\"\"\n",
    "    Train the model with given training set values and other parameters.\n",
    "    :param model: model object with defined architecture.\n",
    "    :param iterator: iterator with batch of text data and corresponding tags.\n",
    "    :param optimizer: optimizer object to be used for training.\n",
    "    :param tag_pad_idx: \n",
    "    :return: per epoch lost and f1 score\n",
    "    \"\"\"\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_f1 = 0\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "\n",
    "        text = batch.text\n",
    "        tags = batch.tag\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_tags_list, batch_loss = model(text, tags)\n",
    "\n",
    "        # to calculate the loss and the score f1, we flatten true tags\n",
    "        true_tags_list = [\n",
    "            [tag for tag in sent_tag if tag != TAG_PAD_IDX]\n",
    "            for sent_tag in tags.permute(1, 0).tolist()\n",
    "        ]\n",
    "        f1,_,_ = compute_f1_loss(pred_tags_list, true_tags_list, tag_pad_idx)\n",
    "\n",
    "        batch_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += batch_loss.item()\n",
    "        epoch_f1 += f1\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_f1 / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, tag_pad_idx,full_report):\n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    :param model: model_object used for trainig.\n",
    "    :param iterator: iterator with batch of training text and tags\n",
    "    :param tag_pad_idx: \n",
    "    :param full_report: \n",
    "    :return: per epoch lost, f1 score, and actual labels\n",
    "    \"\"\"\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_f1 = 0\n",
    "\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in iterator:\n",
    "\n",
    "            text = batch.text\n",
    "            tags = batch.tag\n",
    "\n",
    "            pred_tags_list, batch_loss = model(text, tags)\n",
    "            true_tags_list = [\n",
    "                [tag for tag in sent_tag if tag != TAG_PAD_IDX]\n",
    "                for sent_tag in tags.permute(1, 0).tolist()\n",
    "            ]\n",
    "\n",
    "            f1, pred, lab = compute_f1_loss(pred_tags_list, true_tags_list, tag_pad_idx, full_report)\n",
    "            preds.append(pred)\n",
    "            labels.append(lab)\n",
    "            epoch_loss += batch_loss.item()\n",
    "            epoch_f1 += f1\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_f1 / len(iterator),preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_epoch_time(start_time, end_time):\n",
    "    \"\"\"\n",
    "    Compute the time taken for each epoch\n",
    "    :param start_time: start time of the epoch\n",
    "    :param end_time: end time of the epoch\n",
    "    :return: time in minutes and seconds taken for the epoch\n",
    "    \"\"\"\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "train_loss_list = []\n",
    "train_f1_list = []\n",
    "val_loss_list = []\n",
    "val_f1_list = []\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "   \n",
    "    train_loss, train_f1 = train(model, train_iterator, optimizer_obj, TAG_PAD_IDX)\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_f1_list.append(train_f1) \n",
    "    \n",
    "    valid_loss, valid_f1,_,_ = evaluate(model, validation_iterator, TAG_PAD_IDX, full_report= False)\n",
    "    val_loss_list.append(valid_loss)\n",
    "    val_f1_list.append(valid_f1)\n",
    "    \n",
    "    scheduler_obj.step()\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = calculate_epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'bilstm_crf.pt')\n",
    "    \n",
    "\n",
    "    if epoch%1 == 0: \n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTraining Loss: {train_loss:.3f} | Training F1 score: {train_f1*100:.2f}%')\n",
    "        print(f'\\t Validation Loss: {valid_loss:.3f} |  Validation F1 score: {valid_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting training and Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, N_EPOCHS,N_EPOCHS)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(x,train_loss_list)\n",
    "plt.plot(x,val_loss_list)\n",
    "plt.title(\"Loss\")\n",
    "plt.legend([\"Train loss\", \"Valid loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Training and Validation F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, N_EPOCHS,N_EPOCHS)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(x,train_f1_list)\n",
    "plt.plot(x,val_f1_list)\n",
    "plt.title(\"F1 score\")\n",
    "plt.legend([\"Train F1\", \"Valid F1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the saved model for prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('bilstm_crf.pt'))\n",
    "\n",
    "test_loss, test_f1, preds, labels = evaluate(model, test_iterator, TAG_PAD_IDX, full_report=False)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test F1 score: {test_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict =  [item for sublist in preds for item in sublist]\n",
    "true =  [item for sublist in labels for item in sublist]\n",
    "confusion_mat = confusion_matrix(true, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_values_dataframe =pd.DataFrame(confusion)\n",
    "\n",
    "confusion_values_dataframe.columns=[i for i in tags.vocab.itos]\n",
    "s = pd.Series([i for i in tags.vocab.itos])\n",
    "confusion_values_dataframe = confusion_values_dataframe.set_index([s])\n",
    "\n",
    "confusion_values_dataframe['LOC'] = confusion_values_dataframe['B-LOC'] + confusion_values_dataframe['I-LOC']\n",
    "confusion_values_dataframe['PER'] = confusion_values_dataframe['B-PER'] + confusion_values_dataframe['I-PER']\n",
    "confusion_values_dataframe['ORG'] = confusion_values_dataframe['B-ORG'] + confusion_values_dataframe['I-ORG']\n",
    "confusion_values_dataframe['MISC'] = confusion_values_dataframe['B-MISC'] + confusion_values_dataframe['I-MISC']\n",
    "\n",
    "\n",
    "confusion_values_dataframe = confusion_values_dataframe.drop(columns=[ i for i in tags.vocab.itos if i != 'O'])\n",
    "\n",
    "confusion_values_dataframe.loc['LOC'] = confusion_values_dataframe.loc['B-LOC'] + confusion_values_dataframe.loc['I-LOC']\n",
    "confusion_values_dataframe.loc['PER'] = confusion_values_dataframe.loc['B-PER'] + confusion_values_dataframe.loc['I-PER']\n",
    "confusion_values_dataframe.loc['ORG'] = confusion_values_dataframe.loc['B-ORG'] + confusion_values_dataframe.loc['I-ORG']\n",
    "confusion_values_dataframe.loc['MISC'] = confusion_values_dataframe.loc['B-MISC'] + confusion_values_dataframe.loc['I-MISC']\n",
    "\n",
    "confusion_values_dataframe = confusion_values_dataframe.drop([i for i in tags.vocab.itos if i != 'O'  ])\n",
    "\n",
    "confusion_values_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Precision, Recall, F1Score from Confusion Matrix of the test set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_m = confusion_values_dataframe.to_numpy()\n",
    "\n",
    "TP = np.diag(c_m)\n",
    "FP = np.sum(c_m, axis=0) - TP\n",
    "FN = np.sum(c_m, axis=1) - TP\n",
    "\n",
    "num_classes = 4\n",
    "TN = []\n",
    "for i in range(num_classes):\n",
    "    temp = np.delete(cm, i, 0)    \n",
    "    temp = np.delete(temp, i, 1) \n",
    "    TN.append(sum(sum(temp)))\n",
    "    \n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "f1 = (2*precision*recall)/(precision+recall)\n",
    "\n",
    "prf_df =pd.DataFrame()\n",
    "prf_df['Precision'] = precision\n",
    "prf_df['Recall'] = recall\n",
    "prf_df['F1-score'] = f1\n",
    "\n",
    "s = pd.Series([i for i in confusion_df.index])\n",
    "prf_df = prf_df.set_index([s])\n",
    "\n",
    "prf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the trained models behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sentence(model, device, sentence, text_field, tag_field):\n",
    "    \"\"\"\n",
    "    Used the model to predict the tags for given sentence\n",
    "    :param model: trained model object\n",
    "    :param device: device type either cpu or gpu\n",
    "    :param sentence: sentence to detect the tags in\n",
    "    :param text_field: \n",
    "    :param tag_field: \n",
    "    :return: tokens of the sentence, predicted tags and the unknown.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('en')\n",
    "        sent_tokens = [token.text for token in nlp(sentence)]\n",
    "    else:\n",
    "        sent_tokens = [token for token in sentence]\n",
    "\n",
    "    if text_field.lower:\n",
    "        sent_tokens = [t.lower() for t in sent_tokens]\n",
    "\n",
    "    max_word_len = max([len(token) for token in sent_tokens])\n",
    "\n",
    "    numericalized_tokens = [text_field.vocab.stoi[t] for t in sent_tokens]\n",
    "    unk_idx = text_field.vocab.stoi[text_field.unk_token]\n",
    "    unknowns = [t for t, n in zip(sent_tokens, numericalized_tokens) if n == unk_idx]\n",
    "\n",
    "    token_tensor_obj = torch.as_tensor(numericalized_tokens)\n",
    "    token_tensor_obj = token_tensor_obj.unsqueeze(-1).to(device)\n",
    "\n",
    "    predictions, _ = model(token_tensor_obj)\n",
    "    predicted_tags = [tag_field.vocab.itos[t] for t in predictions[0]]\n",
    "\n",
    "    return sent_tokens, predicted_tags, unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_index = 10\n",
    "\n",
    "sentence = vars(valid_data.examples[example_index])['text']\n",
    "actual_tags = vars(valid_data.examples[example_index])['tag']\n",
    "\n",
    "print(sentence)\n",
    "print(actual_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, pred_tags, unks = tag_sentence(model, \n",
    "                                       device, \n",
    "                                       sentence, \n",
    "                                       text_data, \n",
    "                                       tags\n",
    "                                      )\n",
    "print(pred_tags)\n",
    "print(actual_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used method from Kaggle for better representation in the output cell\n",
    "\n",
    "print(\"Predicted Tag\\t\\t\\t\\tActual Tag\\t\\t\\t\\tCorrect?\\t\\t\\t\\tToken\\n\")\n",
    "for token, pred_tag, actual_tag in zip(tokens, pred_tags, actual_tags):\n",
    "    correct = '✔' if pred_tag == actual_tag else '✘'\n",
    "    space = 5 if pred_tag == 'O'else 4\n",
    "    space1 = 5 if actual_tag == 'O'else 4   \n",
    "    print(pred_tag,\"\\t\"*space, actual_tag, \"\\t\"*space1, correct,\"\\t\"*5, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The Armed Forces of Ukraine has not only minimised Russian gains, but also enabled substantive advances through counter attacks in eastern and southern areas of Ukraine.'\n",
    "tokens, tags, unks = tag_sentence(model, \n",
    "                                  device, \n",
    "                                  sentence, \n",
    "                                  TEXT, \n",
    "                                  TAG\n",
    "                                )\n",
    "\n",
    "print(unks)\n",
    "print(\"Pred. Tag\\tToken\\n\")\n",
    "\n",
    "\n",
    "for token, tag in zip(tokens, tags):\n",
    "    space = 2 if tag == 'O'else 1\n",
    "    print(tag, \"\\t\"*space, token)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "k4LF_qyLKvfK"
   ],
   "name": "4 - Sequence Labeling with Conditional Random Field (CRF) as Output Layer",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
